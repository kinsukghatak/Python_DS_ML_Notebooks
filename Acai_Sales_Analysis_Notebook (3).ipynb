{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Acai CRM Sales Analysis :</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Kinsuk Ghatak\n",
    "#### 9038426200\n",
    "#### kinsukghatak@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <font color='red'>A. Exploratory and Descriptive analysis on the transaction data set </font> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDA Codes : \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "df_acai=pd.read_excel(\"C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/master_data_acai.xlsx\")\n",
    "df_country=pd.read_excel(\"C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/country_code.xlsx\")\n",
    "\n",
    "df_acai=df_acai.merge(df_country.rename({'Country_Name': 'Shipping Country Name'}, axis=1),\n",
    "              left_on='Shipping Country',right_on='Country_Code',how='left')\n",
    "\n",
    "df_acai=df_acai.merge(df_country.rename({'Country_Name': 'Billing Country Name'}, axis=1),\n",
    "              left_on='Billing Country',right_on='Country_Code',how='left')\n",
    "\n",
    "\n",
    "print(\"length of the data set :\",len(df_acai))\n",
    "print(\"Total Columns of the data set :\",len(df_acai.columns))\n",
    "print(\"# of unique customers in the entire tenure from 2016 to 2021 :\",len(df_acai.username.unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pre processing and MVI for rolling up information at transaction level :\n",
    "\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "print(df_acai['Created at'].dtypes)\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "cats = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "cat_type = CategoricalDtype(categories=cats, ordered=True)\n",
    "\n",
    "cats_month = [ 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "cats_month_type = CategoricalDtype(categories=cats_month, ordered=True)\n",
    "\n",
    "cats_year = [ '2016','2017','2018','2019','2020','2021']\n",
    "cats_year_type = CategoricalDtype(categories=cats_year, ordered=True)\n",
    "\n",
    "\n",
    "\n",
    "df_acai['Total'] = pd.to_numeric(df_acai['Total'],errors = 'coerce')\n",
    "\n",
    "\n",
    "df_acai['date_string']=df_acai['Created at'].str[:10]\n",
    "\n",
    "df_acai['Date']  = pd.to_datetime(df_acai['date_string'])\n",
    "\n",
    "# df_acai['month_year'] = df_acai['Date'].dt.strftime('%b-%Y')\n",
    "df_acai['month'] = df_acai['Date'].dt.strftime('%b').astype(cats_month_type)\n",
    "df_acai['year'] = df_acai['Date'].dt.strftime('%Y').astype(cats_year_type)\n",
    "df_acai['month_year']=df_acai['month'].astype(str)+'-'+df_acai['year'].astype(str)\n",
    "\n",
    "df_acai['Day'] = df_acai['Date'].dt.strftime('%A').astype(cat_type)\n",
    "\n",
    "df_acai=df_acai.drop(columns='date_string').copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## missing value filling up :\n",
    "vars_ffill= ['Financial Status','Accepts Marketing','Shipping Method','Payment Method','Risk Level','Source',\n",
    "             'Shipping Country','Billing Country']\n",
    "for col in vars_ffill:\n",
    "    df_acai[col]=df_acai[col].ffill()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_acai.to_csv('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/Processed_Sales_Data.csv')\n",
    "\n",
    "print(\"Total number of unique SKUs %d : \"  % len(df_acai['Lineitem name'].unique()))\n",
    "print(\"Total number of unique Lineitem description %d : \"  % len(df_acai['Lineitem name'].unique()))\n",
    "print(\"Total number of unique Customers %d : \"  % len(df_acai['Name'].unique()))\n",
    "\n",
    "\n",
    "df_acai.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables to keep for analysis :\n",
    "var_list = ['Name','username','year','month_year','month','Date','Day','Created at','Financial Status','Accepts Marketing','Shipping Method','Payment Method','Risk Level','Source',\n",
    "            'Lineitem quantity','Total','Discount Code','Discount Amount','Lineitem price','Billing City','Billing Zip','Billing Province','Billing Country',\n",
    "           'Billing Country Name','Shipping City','Shipping Zip','Shipping Province','Shipping Country','Shipping Country Name',\n",
    "            'Created at' , 'Lineitem name','Lineitem sku','Product_Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Product segment processing and identification : \n",
    "\n",
    "\n",
    "keywords=['trouser','vest','bra','legging','tee','top','sweater','capri','shorts','socks','pants','thermal',\n",
    "             'jeans','wash','care','hoodie','belt','sweatshirt','khaki','tights']\n",
    "\n",
    "pat = '|'.join(r\"\\b{}\\b\".format(x) for x in keywords)\n",
    "\n",
    "df_acai['Category1'] = df_acai['Lineitem name'].str.lower().str.findall(pat).apply(set).str.join(', ')\n",
    "df_acai['Category2'] = df_acai['Lineitem name'].str.lower().str.findall('|'.join(keywords)).apply(set).str.join(', ')\n",
    "\n",
    "df_acai['Product_Category']=df_acai['Category2'].apply(lambda x: x.split(',')[0])\n",
    "\n",
    "# df_acai.loc[df_acai[\"Product_Category\"].isnull(),'Product_Category'] = \"Others\"\n",
    "\n",
    "df_acai.loc[df_acai.Product_Category == '', 'Product_Category'] = 'Others'\n",
    "\n",
    "df_acai.head()\n",
    "\n",
    "df_filtd_colmns=df_acai[var_list].copy()\n",
    "df_filtd_colmns = df_filtd_colmns[df_filtd_colmns['year']!='2016']\n",
    "\n",
    "print(len(df_filtd_colmns))\n",
    "\n",
    "\n",
    "# df_filtd_colmns.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/Processed_Sales_Data_v3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering only those transactions which has some status like paid / refunded etc. We have total 95484 such results and \n",
    "# a frequency table of the same is provided below to better understand the payment sceanario. \n",
    "\n",
    "df_filt=df_filtd_colmns[(df_filtd_colmns['Financial Status'].notnull())]\n",
    "print(len(df_filt))\n",
    "pd.crosstab(df_filtd_colmns['Financial Status'], columns='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_context(\"paper\", rc={\"font.size\":12,\"axes.titlesize\":15,\"axes.labelsize\":15})\n",
    "sns.set_style('white')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_filtd_colmns['year-month']=df_filtd_colmns['Date'].dt.to_period('M')\n",
    "\n",
    "\n",
    "df_filtd_colmns['Lineitem_name_processed']=df_filtd_colmns['Lineitem name'].str.lower().apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_sku_month_yr = df_filtd_colmns.groupby(['year','year-month','month']).agg({'Lineitem_name_processed': 'nunique'}).reset_index().rename(columns={'Lineitem_name_processed': 'Unique counts of SKUs'})\n",
    "\n",
    "# df_sku_month_yr.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/sku_analysis.xlsx')\n",
    "\n",
    "\n",
    "sns.barplot(y='Unique counts of SKUs', x='year-month',data=df_sku_month_yr)\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "plt.title('Month on month change of SKU',fontsize=15,color=\"black\",fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_sku_yr = df_filtd_colmns.groupby(['year']).agg({'Lineitem_name_processed': 'nunique'}).reset_index().rename(columns={'Lineitem_name_processed': 'Unique counts of SKUs'})\n",
    "\n",
    "df_sku_yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find unique increase of SKU counts : \n",
    "\n",
    "df_filtd_colmns = df_filtd_colmns[df_filtd_colmns['year']!='2016']\n",
    "\n",
    "print(\"Total number of unique SKUs %d : \"  % len(df_filtd_colmns['Lineitem name'].unique()))\n",
    "\n",
    "\n",
    "print(\"Total number of unique SKUs in lower font %d : \"  % len(df_filtd_colmns['Lineitem name'].str.lower().apply(lambda x: x.strip()).unique()))\n",
    "\n",
    "\n",
    "df_filtd_colmns['Lineitem_name_processed']=df_filtd_colmns['Lineitem name'].str.lower().apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "df_filtd_colmns['SKU_Code']=pd.factorize(df_filtd_colmns['Lineitem_name_processed'])[0]+1\n",
    "\n",
    "df_sku_max_yr= df_filtd_colmns.groupby(['year']).agg({'SKU_Code':'max'}).reset_index().\\\n",
    "rename(columns={'SKU_Code':'Max_sku_count_year'})\n",
    "\n",
    "df_sku_max_mth= df_filtd_colmns.groupby(['year-month']).agg({'SKU_Code':'max'}).reset_index().\\\n",
    "rename(columns={'SKU_Code':'Max_sku_count_mth'})\n",
    "\n",
    "df_sku_max_yr['New Launches year']= df_sku_max_yr['Max_sku_count_year']- df_sku_max_yr['Max_sku_count_year'].shift(1)\n",
    "\n",
    "\n",
    "df_sku_max_mth['New Launches month']= df_sku_max_mth['Max_sku_count_mth']- df_sku_max_mth['Max_sku_count_mth'].shift(1)\n",
    "\n",
    "df_sku_max_mth.head(2)\n",
    "\n",
    "df_sku_max_yr\n",
    "\n",
    "df_sku_max_mth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Total numbr of unique SKS %d:\" % len(df_filtd_colmns['Lineitem name'].str.lower().apply(lambda x: x.strip()).unique()))\n",
    "\n",
    "print (\"Total numbr of unique customers %d:\" % len(df_filtd_colmns['username'].str.lower().unique()))\n",
    "\n",
    "\n",
    "print (\"Total numbr of unique transactions %d:\" % len(df_filtd_colmns['Name'].str.lower().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sales Movement and trends on month and weekdays : \n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_context(\"paper\", rc={\"font.size\":15,\"axes.titlesize\":25,\"axes.labelsize\":25})  \n",
    "sns.set(font_scale = 1.4)\n",
    "sns.set_style('white')\n",
    "\n",
    "df_full= df_filtd_colmns[df_filtd_colmns['Financial Status']=='paid']\n",
    "\n",
    "cats = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "cats_month = [ 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "cats_month_type = CategoricalDtype(categories=cats_month, ordered=True)\n",
    "\n",
    "df_sales = df_full.groupby(['year','month_year','month','Day']).agg({'Total':'sum','Lineitem quantity':'sum'}).reset_index().rename(columns={'Total': 'Sales Value','Lineitem quantity': 'Sales Volume'})\n",
    "# df_sku_month_yr=df_sku_month_yr[df_sku_month_yr['year']!='2016'].copy()\n",
    "\n",
    "df_sales_day=df_full.groupby(['Day']).agg({'Total':'sum','Lineitem quantity':'sum'}).reset_index().\\\n",
    "rename(columns={'Total': 'Sales Value','Lineitem quantity': 'Sales Volume'}) \n",
    "\n",
    "df_sales_month=df_full.groupby(['month']).agg({'Total':'sum','Lineitem quantity':'sum'}).reset_index().\\\n",
    "rename(columns={'Total': 'Sales Value','Lineitem quantity': 'Sales Volume'}) \n",
    "\n",
    "sns.barplot(y='Sales Value', x='Day',data=df_sales_day).set_title('Weekday wise sales value',\\\n",
    "                                                                  fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(y='Sales Value', x='month',data=df_sales_month).set_title('Month wise sales value',\\\n",
    "                                                                  fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n",
    "plt.show()\n",
    "\n",
    "df_sales_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer retention and basket analysis : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Customer retention analysis : \n",
    "\n",
    "df_username =df_acai[df_acai['year'] !='2016']\n",
    "\n",
    "df_username=df_username[df_username['username'].notnull()]\n",
    "\n",
    "df_username['year_numeric']=df_username['year'].astype(int)\n",
    "df_username.head()\n",
    "\n",
    "\n",
    "for year in (2016,2017,2018,2019,2020,2021):\n",
    "    next_username_list= []\n",
    "    prev_username_list =[]\n",
    "    common_users_set =set()\n",
    "    \n",
    "    year_prev_str=year\n",
    "    year_next_str =year+1\n",
    "    \n",
    "    df_prev= df_username[df_username['year_numeric']<=year_prev_str]\n",
    "    prev_username_list=df_prev['username'].str.lower().unique().tolist()\n",
    "    \n",
    "    df_next = df_username[df_username['year_numeric']==year_next_str]\n",
    "    next_username_list=df_next['username'].str.lower().unique().tolist()\n",
    "    \n",
    "    common_users_set = set(next_username_list) & set(prev_username_list)\n",
    "\n",
    "    print(\"Total number of unique customers for \", year_next_str , \"is %d:\" %len(next_username_list))\n",
    "\n",
    "    print (\"Common Customers between year :\",year_prev_str , 'and' ,year_next_str , 'is %d: '  %len(common_users_set))\n",
    "    \n",
    "print(\"Total number of unique customers for the period : %d \" %len(df_username['username'].str.lower().unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Basket Distribution :\n",
    "## Customer wise finding no of qtys. :\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_context(\"paper\", rc={\"font.size\":15,\"axes.titlesize\":25,\"axes.labelsize\":25})  \n",
    "sns.set(font_scale = 1.4)\n",
    "# sns.set_style('white')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_filtd_colmns['Lineitem_name_processed']=df_filtd_colmns['Lineitem name'].str.lower().apply(lambda x: x.strip())\n",
    "\n",
    "df_full= df_filtd_colmns[df_filtd_colmns['Financial Status']=='paid']\n",
    "\n",
    "\n",
    "df_basket=df_full.groupby(['Name']).agg({'Lineitem_name_processed': pd.Series.nunique}).reset_index().rename(columns={'Lineitem_name_processed': 'Unique Products Bought'})\n",
    "# df_basket.head()\n",
    "df_product_basket_count=df_basket.groupby('Unique Products Bought').agg({'Name':'count'}).reset_index().rename(columns={'Name': 'Total_occurance'})\n",
    "df_product_basket_count['Percentage Occurance'] = round(100*df_product_basket_count['Total_occurance']/df_product_basket_count['Total_occurance'].sum(),2)\n",
    "# df_product_basket_count\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_style('white')\n",
    "sns.set(font_scale = 1.5)\n",
    "sns.barplot(y='Percentage Occurance', x='Unique Products Bought',data=df_product_basket_count).set_title('Product Basket Analysis',\n",
    "                                                                                                  fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##User vs # of transastions distribution\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_context(\"paper\", rc={\"font.size\":15,\"axes.titlesize\":15,\"axes.labelsize\":15})   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_filtd_colmns['Lineitem_name_processed']=df_filtd_colmns['Lineitem name'].str.lower().apply(lambda x: x.strip())\n",
    "\n",
    "df_full= df_filtd_colmns[df_filtd_colmns['Financial Status']=='paid']\n",
    "df_full=df_full[df_full['username'].notnull()]\n",
    "df_full['username']=df_full['username'].astype(str)\n",
    "\n",
    "\n",
    "df_basket=df_full.groupby(['username']).agg({'Name': pd.Series.nunique}).reset_index().rename(columns={'Name': '#Transactions'})\n",
    "\n",
    "df_customer_basket_count=df_basket.groupby('#Transactions').agg({'username':'count'}).reset_index().rename(columns={'username': 'Total_occurance'})\n",
    "df_customer_basket_count['Percentage Occurance'] = round(100*df_product_basket_count['Total_occurance']/df_product_basket_count['Total_occurance'].sum(),2)\n",
    "# df_customer_basket_count\n",
    "\n",
    "df_customer_basket_count\n",
    "\n",
    "# sns.set_style('white')\n",
    "# sns.set(font_scale = 1.5)\n",
    "# sns.barplot(y='Percentage Occurance', x='#Transactions',data=df_customer_basket_count).set_title('Product Basket Analysis',\n",
    "#                                                                                                   fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Product Category analysis : \n",
    "import matplotlib.pyplot as plt\n",
    "df_full= df_filtd_colmns[df_filtd_colmns['Financial Status']=='paid']\n",
    "\n",
    "\n",
    "df_product =df_full.groupby(['Product_Category']).agg({'Total': 'sum', 'Lineitem quantity': 'sum'}).reset_index()\n",
    "df_product.rename(columns = {'Total':'Total Sales Value GBP','Lineitem quantity':'Total Sales Qty'}, inplace = True)\n",
    "\n",
    "\n",
    "df_product['perc_sales_val'] = round(100*df_product['Total Sales Value GBP']/df_product['Total Sales Value GBP'].sum(),2)\n",
    "df_product['perc_sales_vol'] = round(100*df_product['Total Sales Qty']/df_product['Total Sales Qty'].sum(),2)\n",
    "\n",
    "df_product_sorted=df_product.sort_values(by=['perc_sales_val'],ascending=False)\n",
    "df_product_sorted['Cumulative_sales_val_prct']=df_product_sorted['perc_sales_val'].cumsum()\n",
    "df_product_sorted['Cumulative_sales_vol_prct']=df_product_sorted['perc_sales_vol'].cumsum()\n",
    "\n",
    "# print(\"Total number of unique discount strategies %d : \"  % len(cross_rev_disc['Discount Code'].unique()))\n",
    "\n",
    "df_product_sorted['Pie_Chart_Categ'] = np.where(df_product_sorted['Cumulative_sales_val_prct'] > 90 ,\"Other\", df_product_sorted['Product_Category'])\n",
    "df_product_sorted.head(15)\n",
    "\n",
    "df_product_pie =df_product_sorted.groupby(['Pie_Chart_Categ']).agg({'Total Sales Value GBP': 'sum'})\n",
    "\n",
    "df_product_pie.plot.pie(y='Total Sales Value GBP',figsize=(10, 10),legend=False,autopct='%1.1f%%',\\\n",
    "                        startangle=0,textprops={'fontsize': 14})\n",
    "plt.title('Major Category wise revenue distribution',fontsize=20,color=\"black\",fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "## Product category wise unique counts  of SKU :\n",
    "\n",
    "\n",
    "df_prd_sku=df_full.groupby(['Product_Category']).agg({'Lineitem name': pd.Series.nunique}).reset_index().rename(columns={'Lineitem name': 'Unique SKUS Count'})\n",
    "df_prd_sku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Marketing and discounting effectiveness analysis :\n",
    "\n",
    "from matplotlib import pyplot\n",
    "df_discount= df_full.groupby(['Name']).agg({'Total': 'sum','Lineitem quantity' : 'sum','Discount Amount':'sum'}).reset_index().\\\n",
    "rename(columns={'Total': 'Sales Value','Lineitem quantity' : 'Sales Volume'})\n",
    "df_discount.head()\n",
    "\n",
    "sns.relplot(x='Discount Amount', y='Sales Value',data=df_discount)\n",
    "sns.relplot(x='Discount Amount', y='Sales Volume',data=df_discount)\n",
    "\n",
    "# corr=np.corrcoef(df_discount['Discount Amount'], df_discount['Total'])\n",
    "# print('Pearsons correlation: %.3f' % corr)\n",
    "\n",
    "df_discount.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Geography wise analysis : \n",
    "pd.crosstab(index=df_full['Shipping Country'], columns=df_full['Billing Country']).apply(lambda r: r*100/r.sum(), axis=1)\n",
    "\n",
    "##Since in majority of the cases billin=-Shipping ; we will proceed with billing country for country wise splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Bracket Analysis : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Price brakcet distribution :\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_context(\"paper\", rc={\"font.size\":15,\"axes.titlesize\":25,\"axes.labelsize\":25})  \n",
    "sns.set(font_scale = 1.4)\n",
    "sns.set_style('white')\n",
    "\n",
    "\n",
    "df_full['Price Bracket Range']=pd.cut(df_full['Lineitem price'],5)\n",
    "df_full['Price Bracket bin']=pd.cut(df_full['Lineitem price'],5,labels=False)\n",
    "\n",
    "price_bracket_pivot=df_full.groupby(['Price Bracket bin','Price Bracket Range']).agg({'Name':'count','Total':'sum'}).reset_index()\\\n",
    ".rename(columns={'Name':'Count of customers','Total':'Revenue Generated'})\n",
    "\n",
    "price_bracket_pivot['Percentage of customers'] = round(100*price_bracket_pivot['Count of customers']/price_bracket_pivot['Count of customers'].sum(),2)\n",
    "price_bracket_pivot['Percentage of revenue'] = round(100*price_bracket_pivot['Revenue Generated']/price_bracket_pivot['Revenue Generated'].sum(),2)\n",
    "price_bracket_pivot.head()\n",
    "\n",
    "\n",
    "ax = sns.barplot(x=\"Price Bracket bin\", y=\"Percentage of customers\", data=price_bracket_pivot)\n",
    "ax.set(ylabel=\"Percent\")\n",
    "\n",
    "price_bracket_pivot.head()\n",
    "\n",
    "print(price_bracket_pivot)\n",
    "print(df_full['Lineitem price'].describe())\n",
    "\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "price_bracket_pivot\n",
    "\n",
    "\n",
    "##Category wise price analysis : \n",
    "\n",
    "df_price_cat = df_full.groupby('Product_Category').agg({'Lineitem price':[np.mean, np.median,np.max,np.min]})\n",
    "df_price_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Heatmap generettion to observe Category wise price brackets :\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_context(\"paper\", rc={\"font.size\":15,\"axes.titlesize\":25,\"axes.labelsize\":35})  \n",
    "sns.set(font_scale = 1.5)\n",
    "sns.set_style('white')\n",
    "\n",
    "## Price brakcet distribution by product category :\n",
    "\n",
    "func = lambda x: 100*x.count()/df_filtd_colmns.shape[0]\n",
    "\n",
    "df_filtd_colmns['Price Bracket Range']=pd.cut(df_filtd_colmns['Lineitem price'],5)\n",
    "df_filtd_colmns['Price Bracket bin']=pd.cut(df_filtd_colmns['Lineitem price'],5,labels=False)\n",
    "\n",
    "\n",
    "price_bracket_pivot_product = df_filtd_colmns.pivot_table(values='Name',index='Product_Category',columns='Price Bracket bin',aggfunc=func)\n",
    "price_bracket_pivot_product\n",
    "\n",
    "sns.heatmap(price_bracket_pivot_product,annot=True,cmap = 'YlGnBu',cbar_kws={'label': 'Figs in %age'}).set_title('Category wise price brackets',\\\n",
    "                                                                    fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Postal code wise sales value analysis :\n",
    "\n",
    "import folium\n",
    "import pgeocode\n",
    "\n",
    "\n",
    "df_full_uk=df_full[(df_full['Shipping Country'] == \"GB\")]\n",
    "df_sales_zip_uk= df_full_uk.groupby('Shipping Zip').agg({'Name': lambda qty: qty.count()}).sort_values(by=['Name'],ascending=False).reset_index()\n",
    "\n",
    "nomi = pgeocode.Nominatim(\"gb\")\n",
    "\n",
    "dfzip = df_sales_zip_uk[\"Shipping Zip\"].apply(lambda x:nomi.query_postal_code(x))\n",
    "dfzip_final_uk = pd.concat([df_sales_zip_uk, dfzip], axis=\"columns\")\n",
    "dfzip_final_uk.head()\n",
    "#for each row in the df_sales_zip dataset, plot the corresponding latitude and longitude on the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creation of heatmap for Great Britain : \n",
    "\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "dfzip_final_uk_NoNaN=dfzip_final_uk[(dfzip_final_uk['latitude'].notnull()) & (dfzip_final_uk['longitude'].notnull())]\n",
    "\n",
    "map_uk = folium.Map(location=[51.5074, 0.1278], zoom_start = 5) \n",
    "\n",
    "\n",
    "for i,row in dfzip_final_uk_NoNaN.iterrows():\n",
    "    folium.CircleMarker((row.latitude,row.longitude), radius=3, weight=2, color='red', fill_color='red', fill_opacity=.5).add_to(map_uk)\n",
    "\n",
    "    \n",
    "map_uk.add_child(plugins.HeatMap(data=dfzip_final_uk_NoNaN[['latitude', 'longitude','Name']].as_matrix(), radius=15, blur=5))\n",
    "\n",
    "# save the map as an html\n",
    "map_uk.save('map_uk_heatmap.html')\n",
    "map_uk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>B. RFM Segmentation analysis for customer segementation :  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtd_colmns['username_proscsd']=df_filtd_colmns['username'].str.lower()\n",
    "\n",
    "df_filt_user=df_filtd_colmns[df_filtd_colmns['username_proscsd'].notnull()]\n",
    "\n",
    "\n",
    "df_seg= df_filt_user[df_filt_user['Financial Status']=='paid']\n",
    "\n",
    "df_user=df_seg.groupby(['username_proscsd']).agg({'Name':pd.Series.nunique}).rename(columns={'Name' : '#Transactions'}).reset_index()\n",
    "\n",
    "\n",
    "df_seg2=df_seg.merge(df_user,on='username_proscsd',how='left')\n",
    "\n",
    "df_segment_1=df_seg2[df_seg2['#Transactions']==1]\n",
    "df_segment_grt_1=df_seg2[df_seg2['#Transactions']>=2]\n",
    "\n",
    "print(len(df_seg))\n",
    "print(len(df_seg2))\n",
    "print(len(df_segment_1))\n",
    "print(len(df_segment_grt_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate R,F,M Values : \n",
    "\n",
    "from datetime import date\n",
    "today = datetime.now()\n",
    "df_segment_grt_1['Date_copy']=df_segment_grt_1['Date']\n",
    "\n",
    "rfm_df= df_segment_grt_1.groupby('username_proscsd').agg({'Date': lambda date: (today - date.max()).days,\n",
    "                                        'Date_copy': lambda Date: Date.nunique(),\n",
    "                                        'Total': lambda Total: Total.sum()})\n",
    "\n",
    "rfm_df.columns=['recency','frequency','monetary']\n",
    "\n",
    "# rfm_df['monetary']=rfm_df['monetary']/rfm_df['frequency']\n",
    "\n",
    "rfm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying RFM method for segmentation  : \n",
    "\n",
    "from datetime import date\n",
    "import datetime as dt\n",
    "today = datetime.now()\n",
    "\n",
    "\n",
    "# df_segment_grt_1['Date_copy']=df_segment_grt_1['Date']\n",
    "\n",
    "rfm_df= df_segment_grt_1.groupby('username_proscsd').agg({'Date': lambda date: (today - date.max()).days,\n",
    "                                        'Name': lambda cust_id: cust_id.nunique(),\n",
    "                                        'Total': lambda Total: Total.sum()})\n",
    "\n",
    "rfm_df.columns=['recency','frequency','monetary']\n",
    "\n",
    "# rfm_df['monetary_value']=rfm_df['monetary_total']/rfm_df['frequency']\n",
    "\n",
    "# rfm_df.reset_index()\n",
    "\n",
    "rfm_df=rfm_df.dropna()\n",
    "\n",
    "#normalisation of the RFM columns to bring erverything at the same level : \n",
    "# Import library\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Initialize the Object\n",
    "scaler = StandardScaler()\n",
    "# Fit and Transform The Data\n",
    "scaler.fit(rfm_df)\n",
    "rfm_normalized = scaler.transform(rfm_df)\n",
    " \n",
    "\n",
    "rfm_df_normalized = pd.DataFrame(rfm_normalized, index=rfm_df.index, columns=rfm_df.columns)\n",
    "rfm_df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RFM distributions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,10))# Plot distribution of R\n",
    "plt.subplot(3, 1, 1); sns.distplot(rfm_df['recency'])# Plot distribution of F\n",
    "plt.subplot(3, 1, 2); sns.distplot(rfm_df ['frequency'])# Plot distribution of M\n",
    "plt.subplot(3, 1, 3); sns.distplot(rfm_df ['monetary'])# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Box plot for Recency at an over all level : \n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_context(\"paper\", rc={\"font.size\":15,\"axes.titlesize\":25,\"axes.labelsize\":35})  \n",
    "sns.set(font_scale = 1.5)\n",
    "sns.set_style('white')\n",
    "\n",
    "sns.boxplot(y=\"recency\", data=rfm_df)\n",
    "plt.title(\"Distribution of Recency accross all customers who bought more than once\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To analyse and reach optimum number of clusters using k means : \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "sse = {}\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(rfm_df_normalized.dropna())\n",
    "    sse[k] = kmeans.inertia_ # SSE to closest cluster centroid\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('k_No. of clusters')\n",
    "plt.ylabel('SSE')\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post completing the elbow curve it seems that ther curve reaches it's inflection point at K=4 ; i.e post k=4 the rate of change or decrease of SSE w.r.t newer addition of clusters is not that significant and hence we finalise 4 as our final number of segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting the model with k=7 clusters on the rfm dataset : \n",
    "\n",
    "model = KMeans(n_clusters=7, random_state=42)\n",
    "model.fit(rfm_df_normalized.dropna())\n",
    "model.labels_.shape\n",
    "\n",
    "rfm_df[\"kmeans_cluster\"] = model.labels_\n",
    "rfm_df.groupby('kmeans_cluster').agg({\n",
    "    'recency':'mean',\n",
    "    'frequency':'mean',\n",
    "    'monetary':['sum','mean','median', 'count']}).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging all the customer segment information to the original data set : \n",
    "\n",
    "rfm_df=rfm_df.reset_index()\n",
    "rfm_name_clust=rfm_df[['username_proscsd','kmeans_cluster']]\n",
    "rfm_name_clust.head()\n",
    "\n",
    "df_segment_full=df_segment_grt_1.merge(rfm_name_clust, on='username_proscsd', how='left')\n",
    "\n",
    "df_segment_full.head()\n",
    "df_segment_full.groupby('kmeans_cluster').agg({\n",
    "    'username_proscsd':'count'}).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring based on R,F,M percentiles : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RScore(x,p,d):\n",
    "    if x <= d[p][0.20]:\n",
    "        return 5\n",
    "    elif x <= d[p][0.40]:\n",
    "        return 4\n",
    "    elif x <= d[p][0.60]: \n",
    "        return 3\n",
    "    elif x <= d[p][0.80]: \n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    \n",
    "def FMScore(x,p,d):\n",
    "    if x <= d[p][0.20]:\n",
    "        return 1\n",
    "    elif x <= d[p][0.40]:\n",
    "        return 2\n",
    "    elif x <= d[p][0.60]: \n",
    "        return 3\n",
    "    elif x <= d[p][0.80]: \n",
    "        return 4\n",
    "    else:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = rfm_df.quantile(q=[0.20,0.40,0.60,0.80,1])\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate : R , F, M percentile values based on the relative positiosn and alsao create a combined score : \n",
    "\n",
    "rfm_segmentation = rfm_df\n",
    "rfm_segmentation['R_Quartile'] = rfm_segmentation['recency'].apply(RScore, args=('recency',quantiles,))\n",
    "rfm_segmentation['F_Quartile'] = rfm_segmentation['frequency'].apply(FMScore, args=('frequency',quantiles,))\n",
    "rfm_segmentation['M_Quartile'] = rfm_segmentation['monetary'].apply(FMScore, args=('monetary',quantiles,))\n",
    "\n",
    "rfm_segmentation['RFMScore'] = rfm_segmentation.R_Quartile.map(str) \\\n",
    "                            + rfm_segmentation.F_Quartile.map(str) \\\n",
    "                            + rfm_segmentation.M_Quartile.map(str)\n",
    "rfm_segmentation.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution analysis of RFM Summed up score : \n",
    "\n",
    "df_rfm_segmented=df_segment_grt_1.merge(rfm_segmentation,on='username_proscsd',how='left')\n",
    "\n",
    "\n",
    "# Calculate RFM_Score\n",
    "df_rfm_segmented['RFM_Score_sum'] = df_rfm_segmented[['R_Quartile','F_Quartile','M_Quartile']].sum(axis=1)\n",
    "\n",
    "\n",
    "df_rfm_segmented.head()\n",
    "\n",
    "sns.distplot(df_rfm_segmented['RFM_Score_sum'])\n",
    "plt.show()\n",
    "\n",
    "print(df_rfm_segmented['RFM_Score_sum'].describe())\n",
    "\n",
    "quantiles_rfm = df_rfm_segmented['RFM_Score_sum'].quantile(q=[0.20,0.40,0.60,0.80,1])\n",
    "quantiles_rfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rfm_level function\n",
    "## We will segment the customers into different buckets based on their final RFM scummed up score ranges :\n",
    "\n",
    "def rfm_level(df):\n",
    "    if df['RFM_Score_sum'] >= 14:\n",
    "        return 'Can\\'t Loose Them'\n",
    "    elif ((df['RFM_Score_sum'] >= 12) and (df['RFM_Score_sum'] < 14)):\n",
    "        return 'Champions'\n",
    "    elif ((df['RFM_Score_sum'] >= 10) and (df['RFM_Score_sum'] < 12)):\n",
    "        return 'Loyal'\n",
    "    elif ((df['RFM_Score_sum'] >= 8) and (df['RFM_Score_sum'] < 10)):\n",
    "        return 'Potential'\n",
    "    elif ((df['RFM_Score_sum'] >= 6) and (df['RFM_Score_sum'] < 8)):\n",
    "        return 'Promising'\n",
    "    elif ((df['RFM_Score_sum'] >= 4) and (df['RFM_Score_sum'] < 6)):\n",
    "        return 'Needs Attention'\n",
    "    else:\n",
    "        return 'Require Activation'# Create a new variable RFM_Level\n",
    "\n",
    "df_rfm_segmented['RFM_Level'] = df_rfm_segmented.apply(rfm_level, axis=1)# Print the header with top 5 rows to the console\n",
    "df_rfm_segmented.head()\n",
    "\n",
    "# df_rfm_segmented.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/rfm_segmented.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Box plot for Recency distributions at segment levels  : \n",
    "\n",
    " \n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_context(\"paper\", rc={\"font.size\":15,\"axes.titlesize\":25,\"axes.labelsize\":35})  \n",
    "sns.set(font_scale = 1.5)\n",
    "sns.set_style('white')\n",
    "\n",
    "\n",
    "sns.boxplot(y=\"recency\", x=\"RFM_Level\",data=df_rfm_segmented)\n",
    "plt.title(\"Distribution of recency accross different segments of customers who bought more than once\")\n",
    "plt.xticks(rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Finding top 10 customers of top two segments : \n",
    "\n",
    "df_rfm_filtd=df_rfm_segmented[df_rfm_segmented['RFM_Level'].isin(['Can\\'t Loose Them','Champions'])]\n",
    "\n",
    "df_rfm_top_users=df_rfm_filtd.groupby(['RFM_Level','username_proscsd']).agg({'RFM_Score_sum':'max'}).sort_values('RFM_Score_sum').reset_index()\n",
    "\n",
    "df_top_10_users=df_rfm_top_users.sort_values('RFM_Score_sum',ascending = False).groupby('RFM_Level').head(10)\n",
    "\n",
    "top_10_cust_list=df_top_10_users['username_proscsd'].tolist()\n",
    "\n",
    "df_cantloose_champ= df_rfm_filtd[df_rfm_filtd['username_proscsd'].isin(top_10_cust_list)].sort_values(['RFM_Level','RFM_Score_sum'])\n",
    "\n",
    "\n",
    "df_cantloose_champ.head()\n",
    "\n",
    "# df_cantloose_champ.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/df_cantloose_champ.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Segment wise analysis and creating customer profiles with value , volume , recency , frequency , price etc : \n",
    "\n",
    "df_segmented_summary = df_rfm_segmented.groupby('RFM_Level').agg({'Total':'sum','Lineitem quantity':'sum',\\\n",
    "                                                                  'recency':['mean','median'],'frequency':['mean','median'],\\\n",
    "                                                                 'Name':pd.Series.nunique,'Product_Category':pd.Series.mode,\\\n",
    "                                                                 'Lineitem price': ['mean','median'],\\\n",
    "                                                                 'Shipping Zip':pd.Series.mode,\\\n",
    "                                                                 'username_proscsd':pd.Series.nunique,'RFM_Score_sum': ['max','min']}).reset_index().\\\n",
    "rename(columns={'Total':'Sales Value','Lineitem quantity':'Sales Volume','Name':'#Transactions','username_proscsd':'#Unique Customers'})\n",
    "\n",
    "\n",
    "df_segmented_summary\n",
    "\n",
    "df_bubblechart=df_segmented_summary[['RFM_Level','Sales Value','Sales Volume','#Transactions','frequency','recency','#Unique Customers']]\n",
    " \n",
    "df_bubblechart.columns = ['_'.join(col) for col in df_bubblechart.columns.values]\n",
    "\n",
    "\n",
    "df_bubblechart=df_bubblechart.rename(columns={'RFM_Level_':'RFM Segment'})\n",
    "\n",
    "df_bubblechart.head()\n",
    "\n",
    "# df_segmented_summary.head()\n",
    "\n",
    "# df_segmented_summary.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/rfm_segmented_summary.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Visualizing the different segments in a bubble chart and further possibilities of clubbing few segments together : \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_context(\"paper\", rc={\"font.size\":15,\"axes.titlesize\":25,\"axes.labelsize\":35})  \n",
    "sns.set(font_scale = 1.5)\n",
    "sns.set_style('white')\n",
    "\n",
    "\n",
    "g=sns.scatterplot(data=df_bubblechart, x=\"Sales Value_sum\", y=\"frequency_mean\", size=\"#Transactions_nunique\", hue=\"RFM Segment\",alpha=0.5,legend='brief',sizes=(20, 2000))\n",
    "\n",
    "h,l = g.get_legend_handles_labels()\n",
    "plt.legend(h[0:7],l[0:7],bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel(\"Sales Value\")\n",
    "plt.ylabel(\"Mean Frequency\")\n",
    "plt.show(g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RFM Segment wise unique customer counts : \n",
    "\n",
    "ax = sns.barplot(x=\"RFM Segment\", y=\"#Unique Customers\", data=df_bubblechart)\n",
    "ax.set(ylabel=\"Count of customers\")\n",
    "\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## analyiss and profiling for customers who bought only once : \n",
    "\n",
    "from datetime import date\n",
    "today = datetime.now()\n",
    "\n",
    "df_segment_1=df_seg2[df_seg2['#Transactions']==1]\n",
    "df_segment_1['Date_copy']=df_segment_1['Date']\n",
    "\n",
    "rfm_df_1= df_segment_1.groupby('username_proscsd').agg({'Date': lambda date: (today - date.max()).days,\n",
    "                                        'Date_copy': lambda Date: Date.nunique(),\n",
    "                                        'Total': lambda Total: Total.sum(),\\\n",
    "                                        'Lineitem quantity':lambda Total: Total.sum()}).reset_index()\n",
    "\n",
    "rfm_df_1.columns=['username_proscsd','recency','frequency','monetary','Total Qty']\n",
    "df_segment_1_merged=df_segment_1.merge(rfm_df_1,on='username_proscsd',how='left')\n",
    "# rfm_df['monetary']=rfm_df['monetary']/rfm_df['frequency']\n",
    "\n",
    "df_segment_1_merged.head()\n",
    "\n",
    "df_segment_1_merged.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/df_segment_1_merged.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RFM distributions for sigle purchase customers : \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.figure(figsize=(12,10))# Plot distribution of R\n",
    "plt.subplot(3, 1, 1); sns.distplot(rfm_df_1['recency']).set_title('Recency')# Plot distribution of F\n",
    "plt.subplot(3, 1, 2); sns.distplot(rfm_df_1 ['frequency']).set_title('Frequency')# Plot distribution of M\n",
    "plt.subplot(3, 1, 3); sns.distplot(rfm_df_1 ['monetary'])# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# sns.boxplot(y=[\"recency\",\"frequency\",\"monetary\"], data=rfm_df_1)\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(rfm_df_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>C. Market Basket Analysis :  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Basket Distribution :\n",
    "## Customer wise finding no of qtys. :\n",
    "\n",
    "\n",
    "df_filtd_colmns['Lineitem_name_processed']=df_filtd_colmns['Lineitem name'].str.lower().apply(lambda x: x.strip())\n",
    "\n",
    "df_full= df_filtd_colmns[df_filtd_colmns['Financial Status']=='paid']\n",
    "\n",
    "\n",
    "df_basket=df_full.groupby(['Name']).agg({'Lineitem_name_processed': pd.Series.nunique,'Lineitem quantity':'sum'}).reset_index().rename(columns={'Lineitem_name_processed': 'Basket_Size'})\n",
    "\n",
    "\n",
    "df_basket_filtd= df_basket[df_basket['Basket_Size']>1]\n",
    "print(len(df_basket_filtd))\n",
    "df_basket_filtd.head()\n",
    "\n",
    "Trans_list_grt1 = df_basket_filtd['Name'].tolist()\n",
    "print(len(Trans_list_grt1))\n",
    "\n",
    "df_basket_filtd_colmns= df_full[df_full.Name.isin(Trans_list_grt1)]\n",
    "print(len(df_basket_filtd_colmns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_trans=df_basket_filtd_colmns.groupby('Name')['Lineitem_name_processed'].apply(list).reset_index(name='Itemsets')\n",
    "transactions=df_grouped_trans['Itemsets'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket_total=df_basket_filtd_colmns[['Name','Lineitem_name_processed','Lineitem quantity']]\n",
    "# df_item_sku_map=pd.DataFrame(df_full.groupby(['Lineitem name']).size().reset_index(name='Freq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_basket = df_basket_total.groupby(\n",
    "                ['Name', 'Lineitem_name_processed'])['Lineitem quantity']\n",
    "market_basket = market_basket.sum().unstack().reset_index().fillna(0).set_index('Name')\n",
    "\n",
    "market_basket['Basket_Size']= market_basket.sum(axis = 1, skipna = True)\n",
    "market_basket_filtd=market_basket[market_basket['Basket_Size']>1]\n",
    "market_basket_filtd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "market_basket_filtd=market_basket_filtd.drop('Basket_Size',axis=1)\n",
    "print(len(market_basket))\n",
    "print(len(market_basket_filtd))\n",
    "market_basket_filtd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot encoding of the different SKUs to understand which are bought tohgether. \n",
    "\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "basket_sets = market_basket_filtd.applymap(encode_units)\n",
    "# basket_sets.drop('POSTAGE', inplace=True, axis=1)\n",
    "basket_sets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the list of transactions from the dataset\n",
    "transactions = []\n",
    "for i in range(0, len(set(Trans_list_grt1))):\n",
    "    transactions.append([str(market_basket_filtd.values[i,j]) for j in range(0, 2954)])\n",
    "    \n",
    "df_check=df_basket_filtd_colmns.groupby('Name')['Lineitem_name_processed'].apply(list).reset_index(name='Itemsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_trans=df_basket_filtd_colmns.groupby('Name')['Lineitem_name_processed'].apply(list).reset_index(name='Itemsets')\n",
    "\n",
    "\n",
    "transactions=df_grouped_trans['Itemsets'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Apriori algorithm on the dataset\n",
    "# https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python\n",
    "from apyori import apriori\n",
    "rule_list = apriori(transactions, min_support = 0.003, min_confidence = 0.3, min_lift = 3, min_length = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in rule_list:\n",
    "\n",
    "    # first index of the inner list\n",
    "    # Contains base item and add item\n",
    "    pair = item[0] \n",
    "    items = [x for x in pair]\n",
    "    print(\"Rule: \" + items[0] + \" -> \" + items[1])\n",
    "\n",
    "    #second index of the inner list\n",
    "    print(\"Support: \" + str(item[1]))\n",
    "\n",
    "    #third index of the list located at 0th\n",
    "    #of the third index of the inner list\n",
    "\n",
    "    print(\"Confidence: \" + str(item[2][0][2]))\n",
    "    print(\"Lift: \" + str(item[2][0][3]))\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that the data is structured properly, we can generate frequent item sets that have a support of at least 7%\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "frequent_itemsets = fpgrowth(basket_sets, min_support=0.007,use_colnames=True)\n",
    "\n",
    "frequent_itemsets\n",
    "# frequent_itemsets.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/frequent_itemsets.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = df_basket_filtd_colmns.groupby(['Name'])\\\n",
    ".agg({'Lineitem_name_processed': lambda x: x.ravel().tolist()}).reset_index().rename(columns={'Lineitem_name_processed':'itemsets'})\n",
    "\n",
    "items.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "combinations_list = []\n",
    "for row in items.itemsets:\n",
    "    combinations = list(itertools.combinations(row, 2))\n",
    "    combinations_list.append(combinations)\n",
    "\n",
    "combinations_list[:2]\n",
    "\n",
    "combination_counts = pd.Series(combinations_list).explode().reset_index(drop=True)\n",
    "\n",
    "combination_counts[:5]\n",
    "combination_counts.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combination= pd.DataFrame(combination_counts.value_counts())\n",
    "\n",
    "\n",
    "# df_combination.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/mktbskt_comb.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Apriori algorithm on the dataset\n",
    "rule_list = apriori(transactions, min_support = 0.003, min_confidence = 0.3, min_lift = 3, min_length = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=0.0000001)\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full= df_filtd_colmns[df_filtd_colmns['Financial Status']=='paid']\n",
    "df_full['username_proscsd']=df_full['username'].str.lower()\n",
    "\n",
    "# df_check=df_full.groupby(['username_proscsd','Name'])['Date'].first()\n",
    "# df_check.head()\n",
    "\n",
    "df_full_sorted=df_full.sort_values(['username_proscsd', 'Name','Date'], ascending=[True, True,True])\n",
    "\n",
    "\n",
    "df_full_sorted.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/df_full_sorted.xlsx')\n",
    "df_full_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans=df_full_sorted.groupby(['username_proscsd','Name','Date']).agg(Total_qty=('Lineitem quantity','sum'),\\\n",
    "                                                                         Item_List=('Lineitem_name_processed', \\\n",
    "                                                                         lambda x: x.ravel().tolist()),\\\n",
    "                                                                        Unq_qty=('Lineitem_name_processed',lambda x: x.nunique())).reset_index()\n",
    "\n",
    "df_trans_sorted=df_trans.sort_values(['username_proscsd', 'Name','Date'], ascending=[True, True,True])\n",
    "\n",
    "df_trans_sorted['#Purchase_sequence']=df_trans_sorted.groupby(['username_proscsd']).cumcount()+1\n",
    "df_trans_sorted.head()\n",
    "# df_trans_sorted.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/df_trans_sorted.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_1_item=df_trans_sorted[df_trans_sorted['#Purchase_sequence']==1]['Item_List'].tolist()\n",
    "df_trans_1_item_list=[str(item[0]) for item in df_trans_1_item]\n",
    "df_trans_1_item_df=pd.DataFrame(pd.Series(df_trans_1_item_list).value_counts()).reset_index()\n",
    "df_trans_1_item_df.columns=['SKU Name','Count']\n",
    "df_trans_1_item_df['Percentage Occurance'] = round(100*df_trans_1_item_df['Count']/df_trans_1_item_df['Count'].sum(),2)\n",
    "df_trans_1_item_df['#Sequence']=1\n",
    "\n",
    "\n",
    "df_trans_2_item=df_trans_sorted[df_trans_sorted['#Purchase_sequence']==2]['Item_List'].tolist()\n",
    "df_trans_2_item_list=[str(item[0]) for item in df_trans_2_item]\n",
    "df_trans_2_item_df=pd.DataFrame(pd.Series(df_trans_2_item_list).value_counts()).reset_index()\n",
    "df_trans_2_item_df.columns=['SKU Name','Count']\n",
    "df_trans_2_item_df['Percentage Occurance'] = round(100*df_trans_2_item_df['Count']/df_trans_2_item_df['Count'].sum(),2)\n",
    "df_trans_2_item_df['#Sequence']=2\n",
    "\n",
    "df_trans_3_item=df_trans_sorted[df_trans_sorted['#Purchase_sequence']==3]['Item_List'].tolist()\n",
    "df_trans_3_item_list=[str(item[0]) for item in df_trans_3_item]\n",
    "df_trans_3_item_df=pd.DataFrame(pd.Series(df_trans_3_item_list).value_counts()).reset_index()\n",
    "df_trans_3_item_df.columns=['SKU Name','Count']\n",
    "df_trans_3_item_df['Percentage Occurance'] = round(100*df_trans_3_item_df['Count']/df_trans_3_item_df['Count'].sum(),2)\n",
    "df_trans_3_item_df['#Sequence']=3\n",
    "\n",
    "\n",
    "df_trans_4_item=df_trans_sorted[df_trans_sorted['#Purchase_sequence']==4]['Item_List'].tolist()\n",
    "df_trans_4_item_list=[str(item[0]) for item in df_trans_4_item]\n",
    "df_trans_4_item_df=pd.DataFrame(pd.Series(df_trans_4_item_list).value_counts()).reset_index()\n",
    "df_trans_4_item_df.columns=['SKU Name','Count']\n",
    "df_trans_4_item_df['Percentage Occurance'] = round(100*df_trans_4_item_df['Count']/df_trans_4_item_df['Count'].sum(),2)\n",
    "df_trans_4_item_df['#Sequence']=4\n",
    "\n",
    "\n",
    "df_trans_final_top10= pd.concat([df_trans_1_item_df[:10],df_trans_2_item_df[:10],df_trans_3_item_df[:10],df_trans_4_item_df[:10]],axis=0).reset_index(drop=True)\n",
    "\n",
    "df_trans_final_top10.head(2)\n",
    "\n",
    "df_trans1_wide=df_trans_1_item_df.loc[:10,['SKU Name','Percentage Occurance','#Sequence']]\n",
    "df_trans2_wide=df_trans_2_item_df.loc[:10,['SKU Name','Percentage Occurance','#Sequence']]\n",
    "df_trans3_wide=df_trans_3_item_df.loc[:10,['SKU Name','Percentage Occurance','#Sequence']]\n",
    "df_trans4_wide=df_trans_4_item_df.loc[:10,['SKU Name','Percentage Occurance','#Sequence']]\n",
    "\n",
    "\n",
    "df_trans_final_wide= pd.concat([df_trans1_wide,df_trans2_wide,df_trans3_wide,df_trans4_wide],axis=1)\n",
    "df_trans_final_wide.head()\n",
    "\n",
    "\n",
    "# df_trans_final_wide.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/df_trans_final_wide.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Top 10 selling SKUs of each year : \n",
    "df_full= df_filtd_colmns[df_filtd_colmns['Financial Status']=='paid']\n",
    "# df_full2= df_full[df_full['year']!='2016'].copy()\n",
    "df_top_sku=df_full.groupby(['year','Lineitem_name_processed']).agg({'Total':'sum'}).reset_index().\\\n",
    "                                                                sort_values(['year','Total'], ascending=[False,False])\n",
    "\n",
    "# df_top_sku['Percentage Sales'] =df_top_sku.groupby('year').agg({lambda x:x round(100*df_top_sku['Total']/df_top_sku['Total'].sum(),2)})                                   \n",
    "\n",
    "f = lambda x: 100 * x / float(x.sum())\n",
    "df_top_sku['perc_sales'] = (df_top_sku.groupby('year')['Total']\n",
    "                                        .transform(f))\n",
    "\n",
    "df_top_sku.head()\n",
    "\n",
    "# df_full.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/processed_sales_data_fnl.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['year_numeric']=df_full['year'].astype(int)\n",
    "\n",
    "\n",
    "df_stacked_top_10=pd.DataFrame()\n",
    "\n",
    "f = lambda x:  x / float(x.sum())\n",
    "df_top_sku['perc_sales'] = (df_top_sku.groupby('year')['Total']\n",
    "                                        .transform(f))\n",
    "\n",
    "\n",
    "for year in (2017,2018,2019,2020):\n",
    "    \n",
    "    df=pd.DataFrame()\n",
    "    df_top_sku=pd.DataFrame()\n",
    "    df_top_10_sku=pd.DataFrame()\n",
    "    \n",
    "    df=df_full[df_full['year_numeric']==year]\n",
    "    \n",
    "    df_top_sku=df.groupby(['year','Lineitem_name_processed']).agg({'Total':'sum'}).reset_index().\\\n",
    "                                                                sort_values('Total', ascending=False)\n",
    "    df_top_sku['perc_sales'] = (df_top_sku.groupby('year')['Total']\n",
    "                                        .transform(f))\n",
    "\n",
    "    \n",
    "    \n",
    "    df_top_10_sku=df_top_sku[:10]\n",
    "    \n",
    "\n",
    "    \n",
    "    df_stacked_top_10=pd.concat([df_stacked_top_10,df_top_10_sku],axis=0)\n",
    "    \n",
    "\n",
    "df_stacked_top_10\n",
    "\n",
    "df_stacked_top_10.to_excel('C:/Users/KINSUK/Desktop/DS_Freelancing/RTB_Analyttica/Acai_Sales_Analysis_May2021/df_stacked_top_10.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
